{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85e4e9e",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“˜ From Python to Production\n",
    "## Handling Data Efficiently: CSV, JSON, APIs & File System Automation  \n",
    "By **Prerna Joshi** | #25DaysOfDataTech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fdf63c",
   "metadata": {},
   "source": [
    "\n",
    "## What You'll Learn (at a glance)\n",
    "- CSV essentials **and** production patterns: encodings, dialects, chunking, memory safety\n",
    "- JSON and JSON Lines (NDJSON) + flattening nested structures to rows\n",
    "- APIs beyond basics: headers, params, pagination, **retries** and **rate limits**\n",
    "- File system with `pathlib` + safe moves, temp dirs, unique file names\n",
    "- Automation mini-pipelines with **logging** and simple configuration\n",
    "- Practice tasks with solutions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b63ba",
   "metadata": {},
   "source": [
    "## 0) Imports and Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3bfd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 21:26:39,995 | INFO | Notebook started\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, csv, json, time, math, shutil, tempfile, requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, Iterable, List, Tuple, Optional\n",
    "\n",
    "# Minimal, production-leaning logging\n",
    "import logging\n",
    "LOG_DIR = Path(\"logs\"); LOG_DIR.mkdir(exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(LOG_DIR / \"day4.log\", encoding=\"utf-8\")\n",
    "    ],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Notebook started\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc4604",
   "metadata": {},
   "source": [
    "\n",
    "## 1) CSV â€” Bread & Butter of Data Work\n",
    "Key challenges: **encodings, delimiters, quoting, memory**.\n",
    "We'll create sample files so the notebook is self-contained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e652e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created students.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a sample CSV\n",
    "rows = [\n",
    "    [\"name\", \"age\", \"score\"],\n",
    "    [\"Prerna\", \"24\", \"95\"],\n",
    "    [\"Amit\", \"21\", \"88\"],\n",
    "    [\"Riya\", \"22\", \"76\"],\n",
    "    [\"Arjun\", \"23\", \"82\"],\n",
    "]\n",
    "with open(\"students.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(rows)\n",
    "print(\"Created students.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d49729",
   "metadata": {},
   "source": [
    "### 1.1 Reading with `csv.reader` (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "685ac92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'age', 'score']\n",
      "['Prerna', '24', '95']\n",
      "['Amit', '21', '88']\n",
      "['Riya', '22', '76']\n",
      "['Arjun', '23', '82']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"students.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364ac07",
   "metadata": {},
   "source": [
    "### 1.2 Safer: `DictReader` / `DictWriter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193caa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prerna -> age=24, score=95\n",
      "Amit -> age=21, score=88\n",
      "Riya -> age=22, score=76\n",
      "Arjun -> age=23, score=82\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"students.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        print(f\"{row['name']} -> age={row['age']}, score={row['score']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504092d4",
   "metadata": {},
   "source": [
    "### 1.3 Encodings and CSV Dialects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "943f4c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected header: True\n",
      "Dialect delimiter: ,\n",
      "['name', 'age', 'score']\n",
      "['Prerna', '24', '95']\n",
      "['Amit', '21', '88']\n",
      "['Riya', '22', '76']\n",
      "['Arjun', '23', '82']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Detect dialect if file source is unknown (heuristic)\n",
    "with open(\"students.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sample = f.read(1024)\n",
    "    f.seek(0)\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(sample)\n",
    "    except csv.Error:\n",
    "        dialect = csv.excel  # fallback\n",
    "    has_header = csv.Sniffer().has_header(sample)\n",
    "\n",
    "print(\"Detected header:\", has_header)\n",
    "print(\"Dialect delimiter:\", getattr(dialect, \"delimiter\", \",\"))\n",
    "\n",
    "# Use detected dialect to read\n",
    "with open(\"students.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, dialect)\n",
    "    for row in reader:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39234a68",
   "metadata": {},
   "source": [
    "### 1.4 Large Files: Streaming & Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b5306c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 85.25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def iter_csv(path: str) -> Iterable[Dict[str, str]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            yield row\n",
    "\n",
    "def avg_score(path: str) -> float:\n",
    "    total, n = 0, 0\n",
    "    for row in iter_csv(path):\n",
    "        try:\n",
    "            total += int(row[\"score\"])\n",
    "            n += 1\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    return total / n if n else float(\"nan\")\n",
    "\n",
    "print(\"Average score:\", avg_score(\"students.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0e5b11",
   "metadata": {},
   "source": [
    "\n",
    "> **Tip:** When using pandas later, use `dtype`, `usecols`, `chunksize`, and `parse_dates` for speed and memory control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747b371",
   "metadata": {},
   "source": [
    "## 2) JSON & JSON Lines (NDJSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e80ddc",
   "metadata": {},
   "source": [
    "### 2.1 Basic JSON read/write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa756301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config: {'model': 'xgboost', 'learning_rate': 0.05, 'features': ['age', 'amount', 'balance'], 'threshold': 0.82}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\n",
    "    \"model\": \"xgboost\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"features\": [\"age\", \"amount\", \"balance\"],\n",
    "    \"threshold\": 0.82\n",
    "}\n",
    "with open(\"config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(\"config.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded = json.load(f)\n",
    "\n",
    "print(\"Loaded config:\", loaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1825c53b",
   "metadata": {},
   "source": [
    "### 2.2 Nested JSON access + flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70b428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 101, 'user.name': 'Prerna', 'user.country': 'USA', 'transactions': [{'ts': '2025-01-01', 'amount': 120.0}, {'ts': '2025-01-03', 'amount': 199.5}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "sample = {\n",
    "    \"id\": 101,\n",
    "    \"user\": {\"name\": \"Prerna\", \"country\": \"USA\"},\n",
    "    \"transactions\": [\n",
    "        {\"ts\": \"2025-01-01\", \"amount\": 120.0},\n",
    "        {\"ts\": \"2025-01-03\", \"amount\": 199.5},\n",
    "    ],\n",
    "}\n",
    "\n",
    "def flatten(d: Dict[str, Any], parent_key: str = \"\", sep: str = \".\") -> Dict[str, Any]:\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "print(flatten(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5bc470",
   "metadata": {},
   "source": [
    "### 2.3 JSON Lines / NDJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e277c2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'login', 'user': 'alice', 'ok': True},\n",
       " {'event': 'update', 'user': 'bob', 'ok': False, 'reason': 'denied'},\n",
       " {'event': 'logout', 'user': 'alice', 'ok': True}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ndjson_path = \"events.ndjson\"\n",
    "events = [\n",
    "    {\"event\": \"login\", \"user\": \"alice\", \"ok\": True},\n",
    "    {\"event\": \"update\", \"user\": \"bob\", \"ok\": False, \"reason\": \"denied\"},\n",
    "    {\"event\": \"logout\", \"user\": \"alice\", \"ok\": True},\n",
    "]\n",
    "with open(ndjson_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for e in events:\n",
    "        f.write(json.dumps(e, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "parsed = []\n",
    "with open(ndjson_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parsed.append(json.loads(line))\n",
    "parsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3adaaa0",
   "metadata": {},
   "source": [
    "## 3) APIs â€” From Basic to Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb0832",
   "metadata": {},
   "source": [
    "### 3.1 Safe GET with retry & backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75bd2a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User keys (sample): ['login', 'id', 'node_id', 'avatar_url', 'gravatar_id', 'url']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Optional\n",
    "\n",
    "def safe_get(url: str, params: Optional[Dict[str, Any]] = None, headers: Optional[Dict[str, str]] = None,\n",
    "             max_tries: int = 3, timeout: int = 10) -> Optional[Dict[str, Any]]:\n",
    "    tries = 0\n",
    "    while tries < max_tries:\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, headers=headers, timeout=timeout)\n",
    "            if resp.status_code == 429:\n",
    "                retry_after = int(resp.headers.get(\"Retry-After\", \"1\"))\n",
    "                time.sleep(retry_after)\n",
    "                tries += 1\n",
    "                continue\n",
    "            resp.raise_for_status()\n",
    "            return resp.json()\n",
    "        except requests.RequestException as e:\n",
    "            wait = 2 ** tries\n",
    "            print(f\"Request failed ({e}). Retrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "            tries += 1\n",
    "    print(\"Failed after retries:\", url)\n",
    "    return None\n",
    "\n",
    "user = safe_get(\"https://api.github.com/users/prernajoshipj\")\n",
    "print(\"User keys (sample):\", list(user.keys())[:6] if user else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b21d883",
   "metadata": {},
   "source": [
    "### 3.2 Pagination pattern (GitHub public repos, first N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eb39751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': '.github', 'stars': 14},\n",
       " {'name': 'actions', 'stars': 0},\n",
       " {'name': 'click', 'stars': 17047},\n",
       " {'name': 'flask', 'stars': 70888},\n",
       " {'name': 'flask-docs', 'stars': 34}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def fetch_github_repos(user: str, limit: int = 50) -> List[Dict[str, Any]]:\n",
    "    all_repos = []\n",
    "    per_page = 30\n",
    "    page = 1\n",
    "    while len(all_repos) < limit:\n",
    "        data = safe_get(f\"https://api.github.com/users/{user}/repos\", params={\"per_page\": per_page, \"page\": page})\n",
    "        if not data:\n",
    "            break\n",
    "        all_repos.extend(data)\n",
    "        if len(data) < per_page:\n",
    "            break\n",
    "        page += 1\n",
    "    return all_repos[:limit]\n",
    "\n",
    "repos = fetch_github_repos(\"pallets\", limit=40)\n",
    "summary = [{\"name\": r.get(\"name\"), \"stars\": r.get(\"stargazers_count\")} for r in repos[:5]]\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb2497",
   "metadata": {},
   "source": [
    "### 3.3 Save API data (compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0154f4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved gzip JSON -> repos.json.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gzip\n",
    "\n",
    "def save_json_gz(obj: Any, path: Path) -> None:\n",
    "    with gzip.open(path, \"wt\", encoding=\"utf-8\") as gz:\n",
    "        json.dump(obj, gz, ensure_ascii=False)\n",
    "    print(\"Saved gzip JSON ->\", path)\n",
    "\n",
    "if repos:\n",
    "    save_json_gz(repos, Path(\"repos.json.gz\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cc628c",
   "metadata": {},
   "source": [
    "## 4) File System â€” Underrated Production Skill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ea164",
   "metadata": {},
   "source": [
    "### 4.1 Paths, globs, and safe moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d3ae119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved to processed: ['students_backup.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RAW = Path(\"data/raw\"); PROC = Path(\"data/processed\")\n",
    "RAW.mkdir(parents=True, exist_ok=True); PROC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Put some sample files\n",
    "(RAW / \"students_backup.csv\").write_text(Path(\"students.csv\").read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "(RAW / \"notes.txt\").write_text(\"hello\", encoding=\"utf-8\")\n",
    "\n",
    "def move_with_unique(src: Path, dst_dir: Path) -> Path:\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    target = dst_dir / src.name\n",
    "    i = 1\n",
    "    while target.exists():\n",
    "        stem, suf = target.stem, target.suffix\n",
    "        target = dst_dir / f\"{stem}_{i}{suf}\"\n",
    "        i += 1\n",
    "    src.replace(target)\n",
    "    return target\n",
    "\n",
    "moved = []\n",
    "for p in RAW.glob(\"*.csv\"):\n",
    "    moved.append(move_with_unique(p, PROC))\n",
    "\n",
    "print(\"Moved to processed:\", [m.name for m in moved])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5597142",
   "metadata": {},
   "source": [
    "### 4.2 Temp directories for safe pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cf2a2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote atomically -> artifacts\\status.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    tmp = Path(tmpdir)\n",
    "    temp_out = tmp / \"intermediate.json\"\n",
    "    json.dump({\"ok\": True, \"ts\": datetime.now().isoformat()}, open(temp_out, \"w\", encoding=\"utf-8\"))\n",
    "    final = Path(\"artifacts\"); final.mkdir(exist_ok=True)\n",
    "    final_out = final / \"status.json\"\n",
    "    import shutil as _shutil\n",
    "    _shutil.move(str(temp_out), str(final_out))\n",
    "    print(\"Wrote atomically ->\", final_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e36d3",
   "metadata": {},
   "source": [
    "## 5) Automation Mini-Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a347be4d",
   "metadata": {},
   "source": [
    "### 5.1 Daily fetch (weather) with versioned filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cdfea96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data\\weather\\weather_20251207_212643.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fetch_daily_weather(lat=40.7, lon=-74.0) -> Optional[Dict[str, Any]]:\n",
    "    return safe_get(\"https://api.open-meteo.com/v1/forecast\",\n",
    "                    params={\"latitude\": lat, \"longitude\": lon, \"hourly\": \"temperature_2m\"})\n",
    "\n",
    "def save_versioned_json(data: Dict[str, Any], prefix: str, out_dir: Path = Path(\"data/weather\")) -> Path:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    path = out_dir / f\"{prefix}_{ts}.json\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(\"Saved\", path)\n",
    "    return path\n",
    "\n",
    "wx = fetch_daily_weather()\n",
    "if wx:\n",
    "    save_versioned_json(wx, \"weather\")\n",
    "else:\n",
    "    print(\"No weather data (offline or API issue)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61500c87",
   "metadata": {},
   "source": [
    "### 5.2 Folder cleanup (rotate old files, keep N newest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73b3ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rotate_files(folder: Path, pattern: str = \"*.json\", keep: int = 5) -> None:\n",
    "    files = sorted(folder.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    for old in files[keep:]:\n",
    "        old.unlink(missing_ok=True)\n",
    "        print(\"Deleted old:\", old)\n",
    "\n",
    "rotate_files(Path(\"data/weather\"), keep=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db78dd7",
   "metadata": {},
   "source": [
    "## 6) From Nested JSON to Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c328f6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user': {'name': 'Prerna', 'country': 'USA'},\n",
       "  'tx_ts': '2025-01-01',\n",
       "  'tx_amount': 120.0},\n",
       " {'user': {'name': 'Prerna', 'country': 'USA'},\n",
       "  'tx_ts': '2025-01-03',\n",
       "  'tx_amount': 199.5}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "def explode_transactions(record: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    base = {k: v for k, v in record.items() if k != \"transactions\"}\n",
    "    out = []\n",
    "    for t in record.get(\"transactions\", []):\n",
    "        row = base.copy()\n",
    "        row.update({f\"tx_{k}\": v for k, v in t.items()})\n",
    "        out.append(row)\n",
    "    return out\n",
    "\n",
    "rows = explode_transactions({\n",
    "    \"user\": {\"name\": \"Prerna\", \"country\": \"USA\"},\n",
    "    \"transactions\": [{\"ts\": \"2025-01-01\", \"amount\": 120.0},\n",
    "                     {\"ts\": \"2025-01-03\", \"amount\": 199.5}]\n",
    "})\n",
    "rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd2403",
   "metadata": {},
   "source": [
    "### Save flattened rows to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cba85ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote flattened.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def write_dicts_to_csv(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    if not rows:\n",
    "        return\n",
    "    keys = sorted({k for r in rows for k in r.keys()})\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        for r in rows:\n",
    "            writer.writerow(r)\n",
    "\n",
    "write_dicts_to_csv(Path(\"flattened.csv\"), rows)\n",
    "print(\"Wrote flattened.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34012a3",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Practice â€” Apply & Adapt\n",
    "1. Read a large CSV and compute summary stats without loading into memory.  \n",
    "2. Convert NDJSON logs into a single CSV with selected fields.  \n",
    "3. Fetch a paginated API (your choice), collect the first 200 records, and save compressed.  \n",
    "4. Build a small **daily job**: fetch â†’ save versioned â†’ rotate old â†’ append to a master CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec991ee",
   "metadata": {},
   "source": [
    "### âœ… Reference Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "967122fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rows': 4, 'avg_score': 85.25}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def csv_summary(path: str) -> Dict[str, Any]:\n",
    "    cnt = 0; total = 0\n",
    "    for row in iter_csv(path):\n",
    "        try:\n",
    "            total += int(row[\"score\"]); cnt += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {\"rows\": cnt, \"avg_score\": total / cnt if cnt else None}\n",
    "\n",
    "csv_summary(\"students.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b255c2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote events.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ndjson_to_csv(ndjson_file: Path, csv_file: Path, fields: List[str]):\n",
    "    with open(ndjson_file, \"r\", encoding=\"utf-8\") as fin, open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
    "        writer = csv.DictWriter(fout, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "        for line in fin:\n",
    "            obj = json.loads(line)\n",
    "            row = {k: obj.get(k) for k in fields}\n",
    "            writer.writerow(row)\n",
    "\n",
    "ndjson_to_csv(Path(\"events.ndjson\"), Path(\"events.csv\"), [\"event\", \"user\", \"ok\"])\n",
    "print(\"Wrote events.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2355e9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved gzip JSON -> pallets_repos.json.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = fetch_github_repos(\"pallets\", limit=120)\n",
    "save_json_gz(data, Path(\"pallets_repos.json.gz\"))\n",
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f561c7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data\\weather\\weather_20251207_212645.json\n",
      "Daily job executed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def daily_job():\n",
    "    data = fetch_daily_weather()\n",
    "    if not data:\n",
    "        print(\"daily_job: no data\")\n",
    "        return\n",
    "    path = save_versioned_json(data, \"weather\")\n",
    "    rotate_files(path.parent, keep=5)\n",
    "\n",
    "daily_job()\n",
    "print(\"Daily job executed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2088d8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Production Pointers Recap\n",
    "- Prefer **`pathlib`** over raw string paths; write to **temp dirs** then move atomically.  \n",
    "- Replace `print` with **`logging`** in real scripts; keep logs in a dedicated folder.  \n",
    "- For APIs, always set **timeouts**, add **retries**, and handle **rate limits**.  \n",
    "- Use **NDJSON** for streaming logs and large append-only data.  \n",
    "- Version outputs, rotate old files, consider **compression** for large JSON.\n",
    "\n",
    "*Expanded notebook generated on 2025-12-08 02:18:36.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
