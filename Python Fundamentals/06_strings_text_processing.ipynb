{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "17cfd0fd",
      "metadata": {},
      "source": [
        "# From Python to Production\n",
        "## Notebook 6 ‚Äî Strings & Text Processing\n",
        "\n",
        "By **Prerna Joshi** | #25DaysOfDataTech \n",
        "\n",
        "\"Almost every dataset is messy text ‚Äî master strings, and you master real-world data cleaning.\"\n",
        "\n",
        "---\n",
        "\n",
        "### What you'll learn\n",
        "- String basics & immutability, literal forms, escapes, raw strings\n",
        "- Joining, splitting, slicing, searching, replacing\n",
        "- Case handling (`lower`, `title`, `casefold`) and character tests (`isalpha`...)\n",
        "- Unicode & normalization (NFC/NFKD), accent stripping\n",
        "- Bytes vs `str`, encodings, safe file I/O\n",
        "- Regex essentials (`re`): `search`, `findall`, groups, `sub`, flags\n",
        "- Production patterns for text cleaning & extraction\n",
        "- Performance tips for large text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b7f5f1",
      "metadata": {},
      "source": [
        "> **Why this matters for data work**  \n",
        "> 80% of real data wrangling is text cleanup. Knowing the right string tools saves hours and prevents subtle bugs (especially with encodings and Unicode).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "722e6505",
      "metadata": {},
      "source": [
        "## 1. Strings ‚Äî Immutability\n",
        "\n",
        "Python strings are immutable; operations create **new** strings. Prefer `''.join(...)` over repeated `+` in loops.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ac773d86",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('  prerna joshi  ', 'Prerna Joshi', False)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "name = \"  prerna joshi  \"\n",
        "clean = name.strip().title()\n",
        "(name, clean, id(name) == id(clean))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcd22516",
      "metadata": {},
      "source": [
        "## 2. Literals, Escapes, Raw Strings\n",
        "\n",
        "- Single `'...'`, double `\"...\"`, or triple quotes shown as `'''...'''` and `\\\"\\\"\\\"...\\\"\\\"\\\"`\n",
        "- Escape with `\\\\n`, `\\\\t`, `\\\\\\\\`, `\\\\\\\"`\n",
        "- **Raw strings**: prefix with `r` to avoid escape processing (great for regex paths/patterns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a4919d8a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Human-readable (print) ---\n",
            "a:\n",
            "Line1\n",
            "Line2\n",
            "\n",
            "b:\n",
            "C:\\Users\\pj\\projects\\*\\data\n",
            "\n",
            "c, splitlines():\n",
            "Multi-line\n",
            "string with \"quotes\" and 'quotes'\n",
            "\n",
            "--- For comparison, explicit repr() ---\n",
            "repr(a): 'Line1\\nLine2'\n",
            "repr(b): 'C:\\\\Users\\\\pj\\\\projects\\\\*\\\\data'\n",
            "repr(c.splitlines()): ['Multi-line', 'string with \"quotes\" and \\'quotes\\'']\n"
          ]
        }
      ],
      "source": [
        "a = \"Line1\\nLine2\"\n",
        "b = r\"C:\\Users\\pj\\projects\\*\\data\"\n",
        "c = \"Multi-line\\nstring with \\\"quotes\\\" and 'quotes'\"\n",
        "\n",
        "# Debug view (repr) ‚Äî Jupyter shows this for expression results\n",
        "(a, b, c.splitlines())\n",
        "\n",
        "print(\"\\n--- Human-readable (print) ---\")\n",
        "print(\"a:\")\n",
        "print(a)                 # newline renders as a new line\n",
        "print(\"\\nb:\")\n",
        "print(b)                 # backslashes are shown once\n",
        "print(\"\\nc, splitlines():\")\n",
        "for line in c.splitlines():\n",
        "    print(line)\n",
        "\n",
        "print(\"\\n--- For comparison, explicit repr() ---\")\n",
        "print(\"repr(a):\", repr(a))\n",
        "print(\"repr(b):\", repr(b))\n",
        "print(\"repr(c.splitlines()):\", repr(c.splitlines()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Joining & f-Strings (Formatting)\n",
        "\n",
        "- Use `separator.join(iterable)` for efficient concatenation\n",
        "- Prefer **f-strings** for readable formatting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Data ¬∑ AI ¬∑ Engineering', 'Data Engineer: 3 key tasks today')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parts = [\"Data\", \"AI\", \"Engineering\"]\n",
        "joined = \" ¬∑ \".join(parts)\n",
        "role = \"Data Engineer\"; tasks = 3\n",
        "msg = f\"{role}: {tasks} key tasks today\"\n",
        "joined, msg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Splitting / Partitioning / Stripping\n",
        "\n",
        "- `split()` vs `rsplit()`; limit pieces with `maxsplit`\n",
        "- `partition(sep)` returns `(head, sep, tail)` without losing separator\n",
        "- `strip()/lstrip()/rstrip()` remove whitespace or given chars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(('id=101', ';', 'name=Prerna;role=Data Engineer'),\n",
              " ['id=101', 'name=Prerna', 'role=Data Engineer'],\n",
              " 'pj')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = \"id=101;name=Prerna;role=Data Engineer\"\n",
        "left, sep, right = s.partition(\";\")\n",
        "tokens = s.split(\";\")\n",
        "user = \"___pj___\".strip(\"_\")\n",
        "(left, sep, right), tokens, user\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Slicing & Indexing\n",
        "\n",
        "`seq[start:stop:step]` ‚Äî supports negative indices/steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('YTH', 'HON', 'NOHTYP')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"PYTHON\"\n",
        "text[1:4], text[-3:], text[::-1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Searching & Replacing\n",
        "\n",
        "Use `in`, `find`, `index`, `count`, `replace`, `removeprefix`, `removesuffix`, and `translate` for character‚Äëlevel transforms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(True, 5, 'Data | AI | Engineering')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "title = \"  Data‚ÄîAI‚ÄîEngineering  \".strip()\n",
        "has_ai = \"AI\" in title\n",
        "first = title.find(\"AI\")\n",
        "replaced = title.replace(\"‚Äî\", \" | \")\n",
        "has_ai, first, replaced\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Case Handling & Character Tests\n",
        "\n",
        "- `lower/upper/title/capitalize/swapcase`  \n",
        "- `casefold` for aggressive, Unicode‚Äëaware case normalization (useful for search)\n",
        "- `isalpha/isdigit/isalnum/isspace/isdecimal` etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(('stra√üe', 'strasse'), True, True)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = \"Stra√üe\"   # German sharp S\n",
        "(s.lower(), s.casefold()), s.isalpha(), \"123\".isdigit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Unicode & Normalization (NFC/NFKD)\n",
        "\n",
        "Visually identical strings can have different codepoints (composed vs decomposed). Normalize before comparisons or hashing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(False, True)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import unicodedata\n",
        "\n",
        "s1 = \"caf√©\"                          # composed '√©'\n",
        "s2 = \"cafeÃÅ\"                    # 'e' + combining accent\n",
        "eq_raw = (s1 == s2)\n",
        "n1 = unicodedata.normalize(\"NFC\", s1)\n",
        "n2 = unicodedata.normalize(\"NFC\", s2)\n",
        "eq_norm = (n1 == n2)\n",
        "eq_raw, eq_norm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Accent Stripping (when appropriate)\n",
        "\n",
        "Normalize to `NFKD` and drop non‚Äëspacing marks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Curacao, Sao Paulo, Munchen'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import unicodedata\n",
        "\n",
        "def strip_accents(s: str) -> str:\n",
        "    nfkd = unicodedata.normalize(\"NFKD\", s)\n",
        "    return \"\".join(ch for ch in nfkd if unicodedata.category(ch) != \"Mn\")\n",
        "\n",
        "strip_accents(\"Cura√ßao, S√£o Paulo, M√ºnchen\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Bytes vs `str`, Encodings\n",
        "\n",
        "- `str` = Unicode text; `bytes` = raw 8‚Äëbit data\n",
        "- Encode with `.encode('utf-8')`; decode with `.decode('utf-8')`\n",
        "- When reading files, **always** set an explicit encoding (`utf‚Äë8` prefer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('hello ‚ú®', b'hello \\xe2\\x9c\\xa8', 'hello ‚ú®')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t = \"hello ‚ú®\"\n",
        "b = t.encode(\"utf-8\")\n",
        "decoded = b.decode(\"utf-8\")\n",
        "t, b, decoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Safe File I/O (Explicit Encoding)\n",
        "\n",
        "Use `with open(path, encoding=\"utf-8\") as f:` and handle errors with `errors=\"replace\"` or `\"ignore\"` when needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['line 1', 'line 2 ‚ú®']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Demo only; will create & read a temp file\n",
        "path = \"demo_text.txt\"\n",
        "with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"line 1\\nline 2 ‚ú®\")\n",
        "with open(path, encoding=\"utf-8\") as f:\n",
        "    content = f.read()\n",
        "content.splitlines()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Regex Essentials (`re`)\n",
        "\n",
        "- `re.search` (anywhere), `re.match` (start), `re.findall`, `re.sub`\n",
        "- Use raw strings for patterns: `r\"\\d+\"`\n",
        "- Common flags: `re.I` (ignore case), `re.M` (multi-line), `re.S` (dot matches newline)\n",
        "- Groups with `(...)`, named groups `(?P<name>...)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([('prerna.joshi', 'example.com'), ('pj', 'uta.edu')],\n",
              " 'Email me at prerna.joshi@*** or pj@***')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"Email me at prerna.joshi@example.com or pj@uta.edu\"\n",
        "pattern = re.compile(r\"(?P<user>[\\w\\.-]+)@(?P<domain>[\\w\\.-]+)\", re.I)\n",
        "emails = pattern.findall(text)\n",
        "masked = pattern.sub(lambda m: m.group(\"user\") + \"@***\", text)\n",
        "emails, masked\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Common Extraction Patterns (starter)\n",
        "\n",
        "- **Integers / floats**: `r\"-?\\d+\"`, `r\"-?\\d+(?:\\.\\d+)?\"`\n",
        "- **Dates (simple)**: `r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\"`\n",
        "- **Words**: `r\"\\b\\w+\\b\"`\n",
        "> Use libraries (e.g., `dateutil`, `pandas`) for robust parsing in production.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['-12.50', '2025', '-12', '-05', '3'], '2025-12-05')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "s = \"Total: -12.50 on 2025-12-05; items: 3\"\n",
        "floats = re.findall(r\"-?\\d+(?:\\.\\d+)?\", s)\n",
        "date = re.search(r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\", s).group(0)\n",
        "floats, date\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. A Tiny Cleaning Pipeline (Composable)\n",
        "\n",
        "Steps: normalize ‚Üí lower/casefold ‚Üí strip accents ‚Üí collapse whitespace ‚Üí remove punctuation subset ‚Üí tokenize.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cafe‚Äîai', 'engineering', 'is', 'fun', 'right']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re, unicodedata\n",
        "\n",
        "WS = re.compile(r\"\\s+\")\n",
        "PUNCT = str.maketrans({c: \" \" for c in \",.;:!?\"})\n",
        "\n",
        "def normalize_text(s: str) -> list[str]:\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = s.casefold()\n",
        "    s = strip_accents(s)\n",
        "    s = s.translate(PUNCT)\n",
        "    s = WS.sub(\" \", s).strip()\n",
        "    return s.split()\n",
        "\n",
        "normalize_text(\"Caf√©‚ÄîAI, Engineering!!  is FUN\t\tRight?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Performance Tips\n",
        "\n",
        "- Build strings with `join`, not `+` in loops\n",
        "- Pre‚Äëcompile hot regexes: `pattern = re.compile(...)`\n",
        "- Stream files line‚Äëby‚Äëline instead of `read()` for very large files\n",
        "- Use generator pipelines to avoid large intermediates\n",
        "- Normalize once up‚Äëfront when doing many comparisons/hashes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Practice (Try first, then reveal solutions)\n",
        "\n",
        "1. **smart_title**: Implement `smart_title(s)` that title‚Äëcases words but leaves known acronyms (`AI`, `ML`, `NLP`, `USA`) uppercase.  \n",
        "2. **normalize_id**: Given a messy ID string, return lowercase alphanumerics only (drop other chars).  \n",
        "3. **is_palindrome**: Case‚Äë & accent‚Äëinsensitive palindrome check (ignore non‚Äëalphanumerics).  \n",
        "4. **extract_emails**: Return a list of emails from text; mask domains (`user@***`).  \n",
        "5. **word_counts**: Build a frequency dict of tokens using the cleaning pipeline in ¬ß14. Return top 5.  \n",
        "6. **replace_many**: Implement multi‚Äëreplace using `translate` with a mapping `{\",\": \" \", \";\": \" \", \"|\": \" \"}`.  \n",
        "7. **split_kv**: Parse `\"a=1; b=2; c=3\"` into a dict safely using `partition`.  \n",
        "8. **truncate_ellipsis**: Truncate a string to `n` characters without breaking words; add `‚Ä¶` if truncated.  \n",
        "9. **slugify**: Convert `\"Hello, World!\"` ‚Üí `\"hello-world\"` using normalization + regex.  \n",
        "10. **safe_open_read**: Read a file at `path` with `utf-8` and `errors=\"replace\"`, returning the text.  \n",
        "11. **regex_date_swap**: Using regex, transform `\"2025-12-05\"` to `\"12/05/2025\"`.  \n",
        "12. **find_urls** *(bonus)*: Extract basic `http(s)://...` URLs with regex (keep it simple).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. Practice Solutions  \n",
        "*(Click to reveal after solving.)*\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 1Ô∏è‚É£ ‚Äî smart_title</strong></summary>\n",
        "\n",
        "```python\n",
        "ACROS = {\"AI\",\"ML\",\"NLP\",\"USA\"}\n",
        "def smart_title(s: str) -> str:\n",
        "    words = s.split()\n",
        "    out = []\n",
        "    for w in words:\n",
        "        ww = w.upper()\n",
        "        out.append(ww if ww in ACROS else w.title())\n",
        "    return \" \".join(out)\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 2Ô∏è‚É£ ‚Äî normalize_id</strong></summary>\n",
        "\n",
        "```python\n",
        "import re\n",
        "def normalize_id(s: str) -> str:\n",
        "    return re.sub(r\"[^a-z0-9]+\", \"\", s.casefold())\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 3Ô∏è‚É£ ‚Äî is_palindrome</strong></summary>\n",
        "\n",
        "```python\n",
        "import re, unicodedata\n",
        "\n",
        "def _alnum_casefold_no_accent(s: str) -> str:\n",
        "    s = unicodedata.normalize(\"NFKD\", s)\n",
        "    s = \"\".join(ch for ch in s if unicodedata.category(ch) != \"Mn\")\n",
        "    s = s.casefold()\n",
        "    return re.sub(r\"[^a-z0-9]\", \"\", s)\n",
        "\n",
        "def is_palindrome(s: str) -> bool:\n",
        "    t = _alnum_casefold_no_accent(s)\n",
        "    return t == t[::-1]\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 4Ô∏è‚É£ ‚Äî extract_emails</strong></summary>\n",
        "\n",
        "```python\n",
        "import re\n",
        "EMAIL = re.compile(r\"(?P<user>[\\\\w\\\\.-]+)@(?P<domain>[\\\\w\\\\.-]+)\")\n",
        "\n",
        "def extract_emails(text: str):\n",
        "    return EMAIL.findall(text)\n",
        "\n",
        "def mask_domains(text: str):\n",
        "    return EMAIL.sub(lambda m: m.group(\"user\")+\"@***\", text)\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 5Ô∏è‚É£ ‚Äî word_counts</strong></summary>\n",
        "\n",
        "```python\n",
        "from collections import Counter\n",
        "\n",
        "def word_counts(text: str, top=5):\n",
        "    toks = normalize_text(text)\n",
        "    return Counter(toks).most_common(top)\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 6Ô∏è‚É£ ‚Äî replace_many</strong></summary>\n",
        "\n",
        "```python\n",
        "def replace_many(s: str) -> str:\n",
        "    table = str.maketrans({\",\":\" \", \";\":\" \", \"|\":\" \"})\n",
        "    return s.translate(table)\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 7Ô∏è‚É£ ‚Äî split_kv</strong></summary>\n",
        "\n",
        "```python\n",
        "def split_kv(s: str) -> dict:\n",
        "    out = {}\n",
        "    for part in s.split(\";\"):\n",
        "        part = part.strip()\n",
        "        if not part:\n",
        "            continue\n",
        "        k, sep, v = part.partition(\"=\")\n",
        "        if sep:\n",
        "            out[k.strip()] = v.strip()\n",
        "    return out\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 8Ô∏è‚É£ ‚Äî truncate_ellipsis</strong></summary>\n",
        "\n",
        "```python\n",
        "def truncate_ellipsis(s: str, n: int) -> str:\n",
        "    if len(s) <= n:\n",
        "        return s\n",
        "    cut = s[:n].rsplit(\" \", 1)[0]\n",
        "    return cut + \"‚Ä¶\"\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 9Ô∏è‚É£ ‚Äî slugify</strong></summary>\n",
        "\n",
        "```python\n",
        "import re, unicodedata\n",
        "\n",
        "def slugify(s: str) -> str:\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = strip_accents(s).casefold()\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"-\", s).strip(\"-\")\n",
        "    return s\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution üîü ‚Äî safe_open_read</strong></summary>\n",
        "\n",
        "```python\n",
        "def safe_open_read(path: str) -> str:\n",
        "    with open(path, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        return f.read()\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 1Ô∏è‚É£1Ô∏è‚É£ ‚Äî regex_date_swap</strong></summary>\n",
        "\n",
        "```python\n",
        "import re\n",
        "DATE = re.compile(r\"(?P<y>\\\\d{4})-(?P<m>\\\\d{2})-(?P<d>\\\\d{2})\")\n",
        "\n",
        "def date_swap(s: str) -> str:\n",
        "    return DATE.sub(lambda m: f\"{m.group('m')}/{m.group('d')}/{m.group('y')}\", s)\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 1Ô∏è‚É£2Ô∏è‚É£ ‚Äî find_urls</strong></summary>\n",
        "\n",
        "```python\n",
        "import re\n",
        "URL = re.compile(r\"https?://[^\\\\s)]+\")\n",
        "def find_urls(text: str):\n",
        "    return URL.findall(text)\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 18. Mini Cheatsheet\n",
        "\n",
        "- Prefer `join` over `+` in loops\n",
        "- Normalize (`NFC/NFKD`) before comparing/hashing\n",
        "- Use `casefold` for case-insensitive compare\n",
        "- Encode/decode explicitly (`utf-8`); set `encoding=` on `open()`\n",
        "- Pre-compile regexes you reuse; use raw strings for patterns\n",
        "- Build small, composable cleaning functions for clarity & tests\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
