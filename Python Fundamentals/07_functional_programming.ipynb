{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# From Python to Production\n",
        "## Notebook 7 ‚Äî Functional Programming\n",
        "\n",
        "By **Prerna Joshi** | #25DaysOfDataTech \n",
        "\n",
        "\"Functional thinking reduces bugs ‚Äî clean, predictable code scales effortlessly.\"\n",
        "\n",
        "---\n",
        "\n",
        "### What you'll learn\n",
        "- Pure functions, referential transparency, and immutability (as a practice)\n",
        "- Higher‚Äëorder functions: `map`, `filter`, `reduce`, `sorted(key=...)`\n",
        "- Comprehensions vs functional forms (when to pick what)\n",
        "- The `itertools` toolbox for streaming pipelines\n",
        "- Function utilities: `functools.partial`, `lru_cache`, `wraps`\n",
        "- The `operator` module for fast, readable function objects\n",
        "- Generators & iterators; lazy evaluation; back‚Äëpressure friendly design\n",
        "- Composition patterns, error handling, and side‚Äëeffect boundaries\n",
        "- Practical pipelines for data cleaning and analytics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why this matters for data work**  \n",
        "> Functional style reduces hidden state and side effects, making data code more predictable and testable. It pairs well with streaming/large files and ETL.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Pure Functions & Immutability (by discipline)\n",
        "\n",
        "A **pure function** returns the same output for the same input and has no side effects.  \n",
        "Python doesn't enforce immutability, but we can *practice* it by:\n",
        "- Avoiding mutation of inputs\n",
        "- Returning new objects\n",
        "- Keeping I/O at the edges\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.6"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def normalize_score(x: float, mean: float, std: float) -> float:\n",
        "    # pure: no side effects, same input ‚Üí same output\n",
        "    return (x - mean) / std\n",
        "\n",
        "normalize_score(88, mean=80, std=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Higher‚ÄëOrder Functions ‚Äî `map`, `filter`, `sorted(key=...)`, `reduce`\n",
        "\n",
        "Prefer comprehensions for readability; use these when composing lazy pipelines or when a function handle improves clarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([9, 100, 49, 4, 64], [10, 2, 8], 30, [10, 8, 7])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from functools import reduce\n",
        "\n",
        "nums = [3, 10, 7, 2, 8]\n",
        "squared = list(map(lambda x: x*x, nums))\n",
        "even = list(filter(lambda x: x % 2 == 0, nums))\n",
        "total = reduce(lambda a,b: a+b, nums, 0)\n",
        "top3 = sorted(nums, reverse=True)[:3]\n",
        "\n",
        "squared, even, total, top3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Comprehensions vs Functional Forms\n",
        "\n",
        "- Prefer **list/dict/set comprehensions** for simple transforms/filters (more Pythonic).  \n",
        "- Prefer functional forms when you already have named functions or want lazy evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([4, 16, 36], [4, 16, 36])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nums = [1,2,3,4,5,6]\n",
        "comp = [x*x for x in nums if x % 2 == 0]\n",
        "func = list(map(lambda x: x*x, filter(lambda x: x % 2 == 0, nums)))\n",
        "comp, func\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. The `itertools` Toolbox (streaming friendly)\n",
        "\n",
        "- `count`, `cycle`, `repeat` (infinite iterators)\n",
        "- `accumulate`, `chain`, `compress`, `dropwhile`, `takewhile`\n",
        "- `islice`, `tee`, `pairwise`, `groupby`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([1, 3, 6, 10, 15],\n",
              " [(1, 2), (2, 3), (3, 4), (4, 5)],\n",
              " ['a', 'b', 'c', 'd'],\n",
              " {'a': ['a', 'a', 'a'], 'b': ['b', 'b'], 'c': ['c', 'c', 'c']})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from itertools import islice, accumulate, chain, pairwise, groupby\n",
        "\n",
        "nums = [1,2,3,4,5]\n",
        "prefix = list(accumulate(nums))                 # running totals\n",
        "pairs = list(pairwise(nums))                    # adjacent pairs (3.10+)\n",
        "chained = list(chain(\"ab\", \"cd\"))\n",
        "grouped = {k:list(g) for k, g in groupby(\"aaabbccc\")}\n",
        "\n",
        "prefix, pairs, chained, grouped\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. The `operator` Module ‚Äî Faster & Readable Callables\n",
        "\n",
        "Use prebuilt function objects instead of tiny lambdas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'name': 'alice', 'score': 91}, ['alice', 'bob', 'carol'], 42)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import operator as op\n",
        "\n",
        "rows = [\n",
        "    {\"name\":\"alice\",\"score\":91},\n",
        "    {\"name\":\"bob\",\"score\":78},\n",
        "    {\"name\":\"carol\",\"score\":88},\n",
        "]\n",
        "top = max(rows, key=op.itemgetter(\"score\"))\n",
        "names = list(map(op.itemgetter(\"name\"), rows))\n",
        "product = op.mul(6, 7)\n",
        "\n",
        "top, names, product\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. `functools.partial` & Currying (lightweight)\n",
        "\n",
        "Freeze some arguments of a function to make a specialized version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3.0, 0.0)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from functools import partial\n",
        "\n",
        "def scale_and_shift(x, scale=1.0, shift=0.0):\n",
        "    return x * scale + shift\n",
        "\n",
        "stdize = partial(scale_and_shift, scale=1/5, shift=-80/5)  # (x - 80)/5\n",
        "stdize(95), stdize(80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Caching with `lru_cache`\n",
        "\n",
        "Memoize expensive pure-ish functions to speed up repeated calls.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([0, 1, 1, 2, 3, 5, 8, 13, 21, 34],\n",
              " CacheInfo(hits=16, misses=10, maxsize=128, currsize=10))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=128)\n",
        "def fib(n: int) -> int:\n",
        "    if n < 2:\n",
        "        return n\n",
        "    return fib(n-1) + fib(n-2)\n",
        "\n",
        "[fib(i) for i in range(10)], fib.cache_info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Generators & Lazy Evaluation\n",
        "\n",
        "A generator yields items one-by-one and remembers its state. Great for large/streaming data and back‚Äëpressure friendly pipelines.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9]]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def gen_chunks(iterable, size=3):\n",
        "    chunk = []\n",
        "    for x in iterable:\n",
        "        chunk.append(x)\n",
        "        if len(chunk) == size:\n",
        "            yield chunk\n",
        "            chunk = []\n",
        "    if chunk:\n",
        "        yield chunk\n",
        "\n",
        "list(gen_chunks(range(10), size=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Composition & Pipelines\n",
        "\n",
        "Keep I/O at the edges; compose pure transforms in the middle. Small helpers = easier tests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cafe', '‚Äî', 'data', 'ai', 'engineering']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re, unicodedata\n",
        "\n",
        "def strip_accents(s: str) -> str:\n",
        "    nfkd = unicodedata.normalize(\"NFKD\", s)\n",
        "    return \"\".join(ch for ch in nfkd if unicodedata.category(ch) != \"Mn\")\n",
        "\n",
        "WS = re.compile(r\"\\s+\")\n",
        "PUNCT = str.maketrans({c:\" \" for c in \",.;:!?\"})\n",
        "\n",
        "def normalize(s: str) -> str:\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = strip_accents(s).casefold().translate(PUNCT)\n",
        "    s = WS.sub(\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def tokens(s: str):\n",
        "    return (t for t in normalize(s).split() if t)  # generator\n",
        "\n",
        "text = \"Caf√© ‚Äî Data, AI; Engineering!\"\n",
        "list(tokens(text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Error Handling in Pipelines\n",
        "\n",
        "Keep transforms total (defined for all inputs) or isolate edge cases. Use small adapters for validation and fallback.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[10, -1, -1, 30]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def to_int(s, default=None):\n",
        "    try:\n",
        "        return int(s)\n",
        "    except (TypeError, ValueError):\n",
        "        return default\n",
        "\n",
        "values = [\"10\", \"x\", None, \"30\"]\n",
        "converted = list(map(lambda v: to_int(v, default=-1), values))\n",
        "converted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Recursion (and Python's TCO note)\n",
        "\n",
        "Python **does not** perform tail-call optimization. Prefer iteration for deep recursions, or increase recursion limit carefully.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def rec_sum(lst):\n",
        "    if not lst:\n",
        "        return 0\n",
        "    return lst[0] + rec_sum(lst[1:])\n",
        "\n",
        "rec_sum([1,2,3,4])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Decorators (Functional Perspective)\n",
        "\n",
        "Decorators take a function and return a function ‚Äî perfect for cross-cutting concerns (auth, timing, caching).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "slow_pow took 30.13 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "def timer(fn):\n",
        "    @wraps(fn)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        t0 = time.perf_counter()\n",
        "        try:\n",
        "            return fn(*args, **kwargs)\n",
        "        finally:\n",
        "            dt = (time.perf_counter() - t0)*1000\n",
        "            print(f\"{fn.__name__} took {dt:.2f} ms\")\n",
        "    return wrapper\n",
        "\n",
        "@timer\n",
        "def slow_pow(a,b):\n",
        "    time.sleep(0.03)\n",
        "    return a**b\n",
        "\n",
        "slow_pow(2, 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Practical Examples\n",
        "\n",
        "**Example A ‚Äî Streaming CSV rows:** transform, filter bad rows, compute aggregates lazily.  \n",
        "**Example B ‚Äî Top‚Äëk rolling metrics:** use `heapq.nlargest` in a pipeline.  \n",
        "(We use tiny synthetic data here.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'user': 'dave', 'score': 95}, {'user': 'alice', 'score': 91}]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from io import StringIO\n",
        "import csv, heapq\n",
        "\n",
        "CSV = StringIO(\"\"\"user,score\n",
        "alice,91\n",
        "bob,x\n",
        "carol,88\n",
        "dave,95\n",
        "\"\"\")\n",
        "\n",
        "def read_csv_rows(fobj):\n",
        "    r = csv.DictReader(fobj)\n",
        "    for row in r:\n",
        "        yield row\n",
        "\n",
        "def parse_score(row):\n",
        "    try:\n",
        "        row[\"score\"] = int(row[\"score\"])\n",
        "        return row\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "rows = (parse_score(r) for r in read_csv_rows(CSV))\n",
        "valid = (r for r in rows if r is not None)\n",
        "top2 = heapq.nlargest(2, valid, key=lambda r: r[\"score\"])\n",
        "top2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Mini Cheatsheet\n",
        "\n",
        "- Prefer pure, stateless helpers; isolate I/O and side effects\n",
        "- Use comprehensions for concise transforms; `itertools` for streaming\n",
        "- Reach for `operator.itemgetter`, `attrgetter`, `methodcaller` for clarity\n",
        "- Cache pure-ish expensive calls with `@lru_cache`\n",
        "- Compose small functions; test them in isolation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Practice (Try first, then reveal solutions)\n",
        "\n",
        "1. **pipeline_numbers**: Given a list, square only the even numbers and return their sum (try both comprehension and `map/filter/reduce`).  \n",
        "2. **top_k_words**: Given a token stream (iterator), return the top‚Äëk words by frequency lazily (no full list materialization if possible).  \n",
        "3. **moving_avg**: Write a generator `moving_avg(iterable, w)` that yields the windowed average.  \n",
        "4. **compose2**: Implement `compose2(f, g)` that returns a function `h(x)=f(g(x))`.  \n",
        "5. **partial_demo**: Create `to_fixed(base)` via `partial` that formats numbers to `base` decimal places.  \n",
        "6. **safe_map**: Implement `safe_map(fn, iterable, default=None)` that applies `fn` and yields `default` on exceptions.  \n",
        "7. **unique_everseen**: Generator that yields the first time each element appears (like `itertools` recipe).  \n",
        "8. **chunked**: Generator that yields fixed‚Äësize chunks from an iterable.  \n",
        "9. **cached_slow**: Wrap a slow pure function with `lru_cache` and show speedup by calling it repeatedly.  \n",
        "10. **groupby_len**: Using `groupby`, group words by their length (remember to sort first!).  \n",
        "11. **argmax_op**: Using `operator`, find the dict in a list with the largest `\"score\"` key.  \n",
        "12. **normalize_pipeline**: Build a functional text normalize pipeline using `strip_accents` + lower + punctuation removal and return top‚Äë3 tokens by frequency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Practice Solutions  \n",
        "*(Click to reveal after solving.)*\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 1Ô∏è‚É£ ‚Äî pipeline_numbers</strong></summary>\n",
        "\n",
        "```python\n",
        "from functools import reduce\n",
        "# Comprehension\n",
        "def sum_squares_even_comp(nums):\n",
        "    return sum(x*x for x in nums if x % 2 == 0)\n",
        "\n",
        "# map/filter/reduce\n",
        "def sum_squares_even_hof(nums):\n",
        "    return reduce(lambda a,b: a+b, map(lambda x: x*x, filter(lambda x: x%2==0, nums)), 0)\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 2Ô∏è‚É£ ‚Äî top_k_words</strong></summary>\n",
        "\n",
        "```python\n",
        "import heapq\n",
        "from collections import Counter\n",
        "\n",
        "def top_k_words(tokens, k=3):\n",
        "    # Materialize minimal structure via Counter (needs one pass)\n",
        "    c = Counter(tokens)\n",
        "    return heapq.nlargest(k, c.items(), key=lambda kv: kv[1])\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 3Ô∏è‚É£ ‚Äî moving_avg</strong></summary>\n",
        "\n",
        "```python\n",
        "from collections import deque\n",
        "\n",
        "def moving_avg(iterable, w):\n",
        "    d = deque()\n",
        "    s = 0\n",
        "    for x in iterable:\n",
        "        d.append(x); s += x\n",
        "        if len(d) > w:\n",
        "            s -= d.popleft()\n",
        "        if len(d) == w:\n",
        "            yield s / w\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 4Ô∏è‚É£ ‚Äî compose2</strong></summary>\n",
        "\n",
        "```python\n",
        "def compose2(f, g):\n",
        "    def h(x):\n",
        "        return f(g(x))\n",
        "    return h\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 5Ô∏è‚É£ ‚Äî partial_demo</strong></summary>\n",
        "\n",
        "```python\n",
        "from functools import partial\n",
        "\n",
        "def to_fixed(x, base=2):\n",
        "    return f\"{x:.{base}f}\"\n",
        "\n",
        "two_dp = partial(to_fixed, base=2)\n",
        "three_dp = partial(to_fixed, base=3)\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 6Ô∏è‚É£ ‚Äî safe_map</strong></summary>\n",
        "\n",
        "```python\n",
        "def safe_map(fn, iterable, default=None):\n",
        "    for x in iterable:\n",
        "        try:\n",
        "            yield fn(x)\n",
        "        except Exception:\n",
        "            yield default\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 7Ô∏è‚É£ ‚Äî unique_everseen</strong></summary>\n",
        "\n",
        "```python\n",
        "def unique_everseen(iterable):\n",
        "    seen = set()\n",
        "    for x in iterable:\n",
        "        if x not in seen:\n",
        "            seen.add(x)\n",
        "            yield x\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 8Ô∏è‚É£ ‚Äî chunked</strong></summary>\n",
        "\n",
        "```python\n",
        "def chunked(iterable, size):\n",
        "    chunk = []\n",
        "    for x in iterable:\n",
        "        chunk.append(x)\n",
        "        if len(chunk) == size:\n",
        "            yield chunk\n",
        "            chunk = []\n",
        "    if chunk:\n",
        "        yield chunk\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 9Ô∏è‚É£ ‚Äî cached_slow</strong></summary>\n",
        "\n",
        "```python\n",
        "import time\n",
        "from functools import lru_cache\n",
        "\n",
        "def slow_square(x):\n",
        "    time.sleep(0.02)\n",
        "    return x*x\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def cached_square(x):\n",
        "    return slow_square(x)\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution üîü ‚Äî groupby_len</strong></summary>\n",
        "\n",
        "```python\n",
        "from itertools import groupby\n",
        "\n",
        "def groupby_len(words):\n",
        "    words = sorted(words, key=len)\n",
        "    return {k:list(g) for k,g in groupby(words, key=len)}\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 1Ô∏è‚É£1Ô∏è‚É£ ‚Äî argmax_op</strong></summary>\n",
        "\n",
        "```python\n",
        "import operator as op\n",
        "\n",
        "def argmax_score(rows):\n",
        "    return max(rows, key=op.itemgetter(\"score\"))\n",
        "```\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary><strong>Solution 1Ô∏è‚É£2Ô∏è‚É£ ‚Äî normalize_pipeline</strong></summary>\n",
        "\n",
        "```python\n",
        "import re, unicodedata\n",
        "from collections import Counter\n",
        "\n",
        "WS = re.compile(r\"\\\\s+\")\n",
        "PUNCT = str.maketrans({c:\" \" for c in \",.;:!?\"})\n",
        "\n",
        "def strip_accents(s: str) -> str:\n",
        "    nfkd = unicodedata.normalize(\"NFKD\", s)\n",
        "    return \"\".join(ch for ch in nfkd if unicodedata.category(ch) != \"Mn\")\n",
        "\n",
        "def normalize_tokens(s: str):\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = strip_accents(s).casefold().translate(PUNCT)\n",
        "    s = WS.sub(\" \", s).strip()\n",
        "    return (t for t in s.split() if t)\n",
        "\n",
        "def top3_tokens(text):\n",
        "    cnt = Counter(normalize_tokens(text))\n",
        "    return cnt.most_common(3)\n",
        "```\n",
        "</details>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
